sd(est1) # desviacion tipica del estimador
sqrt(1/12)/sqrt(50) # error tipico = desv. del estimador
# Visualizacion del teorema central del limite
hist(est1, freq = FALSE, ylim =c(0,10))
curve(dnorm(x, 0.5, sqrt(1/12)/sqrt(50) ), 0.3, 0.7, add = TRUE, col = 2)
###################################
# ESTIMACION DE LA VARIANZA POBLACIONAL
#Ojo: la varianza poblacional teorica es = 1/12
1/12
# calculamos dos estimadores: varianza muestral y cuasivarianza
var.1 <- apply(datos, 2, var) #cuasivarianza
varp.f <- function(x) mean((x-mean(x))^2) #funcion varianza muestral
var.2 <- apply(datos, 2, varp.f) #varianza muestral
# calculamos las medias de cada estimador
mean(var.1) # la media de la cuasivarianza se aproxima bien al teorico
mean(var.2) # la media de la varianza muestral no se aproxima bien
(50/49)*mean(var.2) # si se multiplica por (n/(n-1)) se aproxima mejor
# Distribucion muestral de la cuasivarianza
chi2 <- 49*var.1/(1/12) # estadistico chi cuadrado
hist(chi2, freq = FALSE)
curve(dchisq(x, 49), 20, 80, col = 2, add = TRUE)
###################################
# DISTRIBUCIONES RELACIONADAS A LA NORMAL
# OJO: uso de prefijos de funciones: d., p., q., r.
###################################
# ESTUDIO DE LOS ESTIMADORES PARA UNA MEDIA
set.seed(1234) #fijamos semillas
# generamos 1000 muestras uniformes (0,1) de tamaño n = 50
datos <- replicate(1000, runif(50))
#histograma de las muestras
hist(datos)
mean(datos)# OJO: la media teórica es el valor de 0.5
sd(datos)
sqrt(1/12) # OJO: varianza teórica (a-b)^2/12
# Consideraremos dos estimadores para la media poblacional teorica
# estimador 1: el promedio
# estimador 2: la mediana
# calculamos ambos estimadores en las 1000 muestras
est1 <- apply(datos, 2, mean)
est2 <- apply(datos, 2, median)
#OJO: se observa que  los estimadores son v.a.
# comparamos los histogramas de los estimadores
windows()
par(mfrow = c(1,2))
hist(est1)
hist(est2)
par(mfrow = c(1,1))
# OJO: ninguno de los estimadores sigue una distribución uniforme (0,1)
# comparamos los estadisticos
mean(est1)
mean(est2)
# los dos estimadores aproximan al parametro = 0.5
# calculamos el sesgo de cada estimador
sesgo1 <- mean(est1) - 0.5
sesgo1
sesgo2 <- mean(est2) - 0.5
sesgo2
# ambos estimadores tiene sesgo casi nulos (est 1 < est 2)
#calculamos la varianza de cada estimador
var1 <- var(est1)
var1
var2 <- var(est2)
var2
# estimador 1 tiene menos variabilidad que el estimador 2
#calculamos el error cuadratico medio
ecm1 <- sesgo1^2 + var1
ecm1
ecm2 <- sesgo2^2 + var2
ecm2
# el estimador 1 (promedio) es mas eficiente que el estimador 2 (mediana)
###################################
# TEOREMA CENTRAL DEL LIMITE
# se tomaron muestras de tamaño n = 50, y se obtuvo el promedio
# en cada muestra. Para este estimador sucede lo siguiente:
mean(est1) # su media "coincide" con la media teorica = 0.5
sd(est1) # desviacion tipica del estimador
sqrt(1/12)/sqrt(50) # error tipico = desv. del estimador
# Visualizacion del teorema central del limite
hist(est1, freq = FALSE, ylim =c(0,10))
curve(dnorm(x, 0.5, sqrt(1/12)/sqrt(50) ), 0.3, 0.7, add = TRUE, col = 2)
###################################
# ESTIMACION DE LA VARIANZA POBLACIONAL
#Ojo: la varianza poblacional teorica es = 1/12
1/12
# calculamos dos estimadores: varianza muestral y cuasivarianza
var.1 <- apply(datos, 2, var) #cuasivarianza
varp.f <- function(x) mean((x-mean(x))^2) #funcion varianza muestral
var.2 <- apply(datos, 2, varp.f) #varianza muestral
# calculamos las medias de cada estimador
mean(var.1) # la media de la cuasivarianza se aproxima bien al teorico
mean(var.2) # la media de la varianza muestral no se aproxima bien
(50/49)*mean(var.2) # si se multiplica por (n/(n-1)) se aproxima mejor
# Distribucion muestral de la cuasivarianza
chi2 <- 49*var.1/(1/12) # estadistico chi cuadrado
hist(chi2, freq = FALSE)
curve(dchisq(x, 49), 20, 80, col = 2, add = TRUE)
gl = ((((5.6^2)-50)+(6.3^2)/50))^2 / (((5.6^2)/50)/49 + ((6.3^2)/50)/49)
gl
gl = ((((5.6^2)/50)+(6.3^2)/50))^2 / (((5.6^2)/50)/49 + ((6.3^2)/50)/49)
gl
gl = ceiling(gl)
gl
tmin = qt(0.025, gl)
tmax = qt(0.095, gl)
tmin; tmax
tmin = qt(0.025, gl)
tmax = qt(0.975, gl)
tmin; tmax
error = tmax * sqrt((5.6^2)/50 + (6.3^2)/50)
error
LIC = (78.3 - 87.2) - err
LIC = (78.3 - 87.2) - error
LSC = (78.3 - 87.2) + error
LIC, LSC
LIC; LSC
n1=12
prom1 = 37900
s1 = 5100
n2 = 12
prom2 = 39800
s2 = 5900
var.c = ((n1-1)*s1^2 + (n2-1)*s2^2) / (n1 +n2 - 2)
var.c
desv.c = sqrt(var.c)
desv.c
t_prom
t_prom = (prom1 - prom2) / (desv.c * sqrt(1/n1 + 1/n2))
t_prom
pt(t_prom, n1+n2-2)
pvalor= 2 * pt(t_prom, n1+n2-2)
pvalor
n = 200 #votantes
exito = 110
prop_muestral = 100/200
prop_muestral
n = 200 #votantes
exito = 110
prop_muestral = n/200
prop_muestral
n = 200 #votantes
exito = 110
prop_muestral = exito/n
prop_muestral
ztest = (p-p0)/sqrt(p0*(1-p0)/n)
n = 200 #votantes
exito = 110
prop_muestral = exito/n
p0=0.6
ztest = (p-p0)/sqrt(p0*(1-p0)/n)
ztest = (prop_muestral-p0)/sqrt(p0*(1-p0)/n)
ztest
Pvalor = pnorm(ztest)
Pvalor
n1 = 100
x1 = 63 #existos
p_1 = x1/n1
p_1
n2 = 125
x2 = 59 #exitos
p_2 = x2/n2
p_2
n3= 20
exito2 = 9
p_muestral = exito2/n3
p_0=0.4
ztest2 = (p_muestral-p_0)/sqrt(p_0*(1-p_0)/0.4)
ztest2
pvalor = pnorm(ztest2)
pvalor
2*pvalor
n3= 20
exito2 = 9
p_muestral = exito2/n3
p_0=0.4
ztest2 = (p_muestral-p_0)/sqrt(p_0*(1-p_0)/p_0)
ztest2
library(DT)
install.packages(DT)
install.packages("DT")
install.packages("dplyr")
install.packages("tidyr")
install.packages("ggplot2")
install.packages("markdown")
library(shiny)
library(DT)
library(DT)
library(dplyr)
library(tidyr)
library(ggplot2)
library(ggplot2)
library(markdown)
install.packages(c("cachem", "cli", "colorspace", "crayon", "digest", "evaluate", "fastmap", "highr", "knitr", "openssl", "PKI", "Rcpp", "rlang", "rmarkdown", "rsconnect", "tinytex", "xfun", "yaml"))
shiny::runApp('C:/Users/EDWARICHSNAKE/Desktop/Estadistica/Unidad_2_Proyecto')
runApp('C:/Users/EDWARICHSNAKE/Desktop/Estadistica/Unidad_2_Proyecto')
runApp('C:/Users/EDWARICHSNAKE/Desktop/Estadistica/Unidad_2_Proyecto')
runApp('C:/Users/EDWARICHSNAKE/Desktop/Estadistica/Unidad_2_Proyecto')
runApp('C:/Users/EDWARICHSNAKE/Desktop/Estadistica/Unidad_2_Proyecto')
runApp('C:/Users/EDWARICHSNAKE/Desktop/Estadistica/Unidad_2_Proyecto')
runApp('C:/Users/EDWARICHSNAKE/Desktop/Estadistica/Unidad_2_Proyecto')
runApp('C:/Users/EDWARICHSNAKE/Desktop/Estadistica/Unidad_2_Proyecto')
runApp('C:/Users/EDWARICHSNAKE/Desktop/Estadistica/Unidad_2_Proyecto')
runApp('C:/Users/EDWARICHSNAKE/Desktop/Estadistica/Unidad_2_Proyecto')
runApp('C:/Users/EDWARICHSNAKE/Desktop/Estadistica/Unidad_2_Proyecto')
runApp('C:/Users/EDWARICHSNAKE/Desktop/Estadistica/Unidad_2_Proyecto')
runApp('C:/Users/EDWARICHSNAKE/Desktop/Estadistica/Unidad_2_Proyecto')
shiny::runApp('C:/Users/EDWARICHSNAKE/Desktop/Estadistica/Unidad_2_Proyecto')
runApp('C:/Users/EDWARICHSNAKE/Desktop/Estadistica/Unidad_2_Proyecto')
shiny::runApp('C:/Users/EDWARICHSNAKE/Desktop/Estadistica/Unidad_2_Proyecto')
shiny::runApp('C:/Users/EDWARICHSNAKE/Desktop/Estadistica/Unidad_2_Proyecto')
shiny::runApp('C:/Users/EDWARICHSNAKE/Desktop/Estadistica/Unidad_2_Proyecto')
################
# REGRESION LINEAL MULTIPLE
#################
# CARGAR LOS DATOS
y = c(240,165,190,274,301) #
x1 = c(25,31,45,60,65) #
x2 = c(91,90,88,87,91) #
n = 5 # total de observaciones
p = 3 # total de coeficientes betas a estimar
# Construir las matrices
Y = matrix(y, nrow = 5) # vector Y
Y
X = matrix(c(rep(1,n),x1,x2), ncol = p) # matriz de diseño
X
# Construir el sistema de ecuaciones normales A*beta = b
t(X) # transpuesta de X
A = t(X) %*% X # Ojo: multiplicacion matricial = %*%
A
b = t(X) %*% Y
b
Ainv = solve(A) # matriz inversa de A
Ainv
Beta = Ainv %*% b # estimacion por MCO del vector Beta
Beta
# Proyecciones de Y
Yproy = X %*% Beta
Yproy
# Residuos
resid = Y - Yproy
resid
# sumatorias de residuos
sum(resid) # muy cercano a cero!
SCR = sum(resid^2)
SCR
t(resid)%*%resid # SCR de forma matricial
# Estimacion de la varianza del error (varianza de los residuos)
sigma2 = SCR / (n-p)
sigma2
sigma = sqrt(sigma2)
sigma
# Matriz de proyeccion H
H = X %*% Ainv %*% t(X) # matriz de proyeccion H
H
H %*% Y # vector de proyecciones de Y
# Matriz generadora de residuos
diag(n)
M = (diag(n) - H) # matriz identidad de tamaño n = diag(n)
M
M %*% Y # vector de residuos
t(H) %*% M
################
# REGRESION LINEAL MULTIPLE
#################
# CARGAR LOS DATOS
y = c(240,165,190,274,301) #
x1 = c(25,31,45,60,65) #
x2 = c(91,90,88,87,91) #
n = 5 # total de observaciones
p = 3 # total de coeficientes betas a estimar
# Construir las matrices
Y = matrix(y, nrow = 5) # vector Y
Y
X = matrix(c(rep(1,n),x1,x2), ncol = p) # matriz de diseño
X
# Construir el sistema de ecuaciones normales A*beta = b
t(X) # transpuesta de X
A = t(X) %*% X # Ojo: multiplicacion matricial = %*%
A
b = t(X) %*% Y
b
# Estimar el vector Beta
Ainv = solve(A) # matriz inversa de A
Ainv
Beta = Ainv %*% b # estimacion por MCO del vector Beta
Beta
# Proyecciones de Y
Yproy = X %*% Beta
Yproy
# Residuos
resid = Y - Yproy
resid
# sumatorias de residuos
sum(resid) # muy cercano a cero!
SCR = sum(resid^2)
SCR
t(resid)%*%resid # SCR de forma matricial
# Estimacion de la varianza del error (varianza de los residuos)
sigma2 = SCR / (n-p)
sigma2
sigma = sqrt(sigma2)
sigma
# Matriz de proyeccion H
H = X %*% Ainv %*% t(X) # matriz de proyeccion H
H
H %*% Y # vector de proyecciones de Y
# Matriz generadora de residuos
diag(n)
M = (diag(n) - H) # matriz identidad de tamaño n = diag(n)
M
M %*% Y # vector de residuos
t(H) %*% M
= %*%
shiny::runApp('C:/Users/EDWARICHSNAKE/Desktop/Estadistica/Unidad_2_Proyecto')
runApp('C:/Users/EDWARICHSNAKE/Desktop/Estadistica/Unidad_2_Proyecto')
runApp('C:/Users/EDWARICHSNAKE/Desktop/Estadistica/Unidad_2_Proyecto')
runApp('C:/Users/EDWARICHSNAKE/Desktop/Estadistica/Unidad_2_Proyecto')
runApp('C:/Users/EDWARICHSNAKE/Desktop/Estadistica/Unidad_2_Proyecto')
runApp('C:/Users/EDWARICHSNAKE/Desktop/Estadistica/Unidad_2_Proyecto')
setwd("C:/Users/EDWARICHSNAKE/Desktop/Estadistica")
# x = Runsize (unidades), y = Runtime (min)
x = c(10, 10, 10, 10, 10, 50, 50, 50, 50, 50)
y = c(13, 18, 16, 15, 20, 86, 90, 88, 88, 82)
n = 10
plot(x, y)
vecmed = c(mean(x), mean(y))
vecmed
Sxy = sum ((x - mean(x))*(y - mean(y))) / n
Sxy
Sxy = sum(x*y)/n - mean(x)*mean(y)
Sxy
S2x =  sum ((x - mean(x))^2) / n
S2x
Beta1 = Sxy / S2x
Beta1
Beta0 = mean(y) - Beta1 * mean(x)
Beta0
Yproy = Beta0 + Beta1*x
Yproy
resid = y - Yproy
resid
plot(x, y)
points(x, Yproy, type="l", col =2)
sum(resid)
etB0 = sigma * sqrt((1/n)+((mean(x)^2)/(n*S2x)))
etB0 # estimacion del error típico del beta cero
tB0 = Beta0 / etB0
# CARGAR LOS DATOS
# x = Runsize (unidades), y = Runtime (min)
x = c(10, 10, 10, 10, 10, 50, 50, 50, 50, 50)
y = c(13, 18, 16, 15, 20, 86, 90, 88, 88, 9)
n = 10  # total de pares ordenados (X, Y)
# Grafico de dispersion
plot(x, y)
# Vector de medias
vecmed = c(mean(x), mean(y))
vecmed
# Covarianza muestral
Sxy = sum ((x - mean(x))*(y - mean(y))) / n
Sxy
Sxy = sum(x*y)/n - mean(x)*mean(y)
Sxy
# Covarianza cuadrada de x
S2x =  sum ((x - mean(x))^2) / n
S2x
# ESTIMACION DE LOS BETAS
# Para Beta 1
Beta1 = Sxy / S2x
Beta1
# Para Beta 0
Beta0 = mean(y) - Beta1 * mean(x)
Beta0
# Proyecciones de Y
Yproy = Beta0 + Beta1*x
Yproy
# Residuos de la regresión
resid = y - Yproy
resid
# Grafico de la recta de proyeccion
plot(x, y)
points(x, Yproy, type="l", col =2)
# Suma de residuos
sum(resid) # practicamente CERO
# Suma cuadrada de residuos: SCR
SCR = sum(resid^2)
SCR
# Estimacion varianza del error
sigma2 = SCR / (n-2)
sigma2
sigma = sqrt(sigma2) # desviación estándar de residuos
sigma
# INFERENCIA PARA LOS BETAS
# Para beta Cero
etB0 = sigma * sqrt((1/n)+((mean(x)^2)/(n*S2x)))
etB0 # estimacion del error típico del beta cero
tB0 = Beta0 / etB0
tB0 # estadistico de contraste de Beta 0
PvalorB0 = 2*(1 - pt(tB0, n-2))
PvalorB0 # es menor al 5% , H0 se rechaza (Beta 0 es distinto de 0)
# Para Beta 1
etB1 = sigma / sqrt(n*S2x)
etB1
tB1 = Beta1 / etB1
tB1
PvalorB1 = 2*(1 - pt(tB1, n-2))
PvalorB1 # es menor al 5% , H0 se rechaza (Beta 1 es distinto de 0)
modelo <- lm(y ~ x)
summary(modelo)
x <- c(10, 10, 10, 10, 10, 50, 50, 50, 50, 50)
y <- c(13, 18, 16, 15, 20, 86, 90, 88, 88, 92)
n <- length(x)  # total de pares ordenados (X, Y)
# GRAFICO DE DISPERSION
plot(x, y, main="Diagrama de dispersión", xlab="Presión (x)", ylab="Lectura de la escala (y)")
# COVARIANZA Y VARIANZA
Sxy <- sum((x - mean(x)) * (y - mean(y))) / n
S2x <- sum((x - mean(x))^2) / n
# ESTIMACION DE LOS BETAS
Beta1 <- Sxy / S2x  # Pendiente
Beta0 <- mean(y) - Beta1 * mean(x)  # Intercepto
# PROYECCIONES DE Y
Yproy <- Beta0 + Beta1 * x
# GRAFICO DE LA RECTA DE REGRESION
plot(x, y, main="Regresión Lineal Simple", xlab="Presión (x)", ylab="Lectura de la escala (y)")
abline(Beta0, Beta1, col="red")
# RESIDUOS DE LA REGRESION
resid <- y - Yproy
# RESUMEN DEL MODELO DE REGRESION LINEAL SIMPLE USANDO lm
RSL1 <- lm(y ~ x)
summary(RSL1)
# PREDICCION EN RLS
x0 <- 200
y0 <- Beta0 + Beta1 * x0
y0
y = c(5.2,4.7,8.1,6.2,3.0,9.1,7.1,8.2,6.0,9.1,3.2,5.8,2.2,3.1,7.2,2.4,3.4,4.1,1.0,4.0,7.1,6.6,9.3,4.2,7.6)
x = c(1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,5,5,5,5) #codificando X como numerica (es categorica)
x = as.factor(x) # funcion as.factor
library(PASWR)
oneway.plots(y, x)
y = c(5.2,4.7,8.1,6.2,3.0,9.1,7.1,8.2,6.0,9.1,3.2,5.8,2.2,3.1,7.2,2.4,3.4,4.1,1.0,4.0,7.1,6.6,9.3,4.2,7.6)
x = c(1,1,1,1,1,2,2,2,2,2,3,3,3,3,3,4,4,4,4,4,5,5,5,5,5) #codificando X como numerica (es categorica)
# convertir X en categorica
x = as.factor(x) # funcion as.factor
# Analisis exploratorio ANOVA
library(PASWR)
oneway.plots(y, x)
modelo = aov(y ~ x) # funcion ANOVA
summary(modelo)
library(MASS)
r = stdres(modelo) # funcion para calcular residuos del ANOVA
# Normalidad
shapiro.test(r)
# Homocedasticidad (levene)
library(lawstat)
levene.test(y, x)
# Independencia
Box.test(r, lag=1, type="Ljung-Box")
library(asbio)
pairw.anova(x = x, y = y, method ="bonf") # Bonferroni
pairw.anova(x = x, y = y, method ="tukey") # Tukey
pairw.anova(x = x, y = y, method ="scheffe") # Scheffe
y <- c(5.2,4.7,8.1,6.2,3.0,9.1,7.1,8.2,6.0,9.1,3.2,5.8,2.2,3.1,7.2,2.4,3.4,4.1,1.0,4.0,7.1,6.6,9.3,4.2,7.6)
x <- factor(c(1,1,1,1,1,2,2,2,2,2,3,3,3,3,3,4,4,4,4,4,5,5,5,5,5)) # Convertir a factor
# Análisis exploratorio ANOVA
library(PASWR)
oneway.plots(y, x)
# Modelo ANOVA
modelo <- aov(y ~ x)
summary(modelo)
# Comprobación de supuestos
library(MASS)
r <- stdres(modelo)
# Normalidad
shapiro.test(r)
# Homocedasticidad
library(lawstat)
levene.test(y, x)
# Independencia
Box.test(r, lag=1, type="Ljung-Box")
######################
# Comparaciones múltiples (Solo si se rechaza H0 en ANOVA)
library(asbio)
pairw.anova(x = x, y = y, method ="tukey") # Método de Tukey como ejemplo
y <- c(5.2,4.7,8.1,6.2,3.0,9.1,7.1,8.2,6.0,9.1,3.2,5.8,2.2,3.1,7.2,2.4,3.4,4.1,1.0,4.0,7.1,6.6,9.3,4.2,7.6)
x <- factor(c(1,1,1,1,1,2,2,2,2,2,3,3,3,3,3,4,4,4,4,4,5,5,5,5,5)) # Convertir a factor
library(PASWR)
oneway.plots(y, x)
modelo <- aov(y ~ x)
summary(modelo)
# Independencia
Box.test(r, lag=1, type="Ljung-Box")
CARGAR LOS DATOS
x <- c(10, 10, 10, 10, 10, 50, 50, 50, 50, 50)
y <- c(13, 18, 16, 15, 20, 86, 90, 88, 88, 92)
n <- length(x)  # total de pares ordenados (X, Y)
# GRAFICO DE DISPERSION
plot(x, y, main="Diagrama de dispersión", xlab="Presión (x)", ylab="Lectura de la escala (y)")
# COVARIANZA Y VARIANZA
Sxy <- sum((x - mean(x)) * (y - mean(y))) / n
S2x <- sum((x - mean(x))^2) / n
# ESTIMACION DE LOS BETAS
Beta1 <- Sxy / S2x  # Pendiente
Beta0 <- mean(y) - Beta1 * mean(x)  # Intercepto
Yproy <- Beta0 + Beta1 * x
# GRAFICO DE LA RECTA DE REGRESION
plot(x, y, main="Regresión Lineal Simple", xlab="Presión (x)", ylab="Lectura de la escala (y)")
abline(Beta0, Beta1, col="red")
resid <- y - Yproy
RSL1 <- lm(y ~ x)
summary(RSL1)
# PREDICCION EN RLS
x0 <- 200
y0 <- Beta0 + Beta1 * x0
y0
